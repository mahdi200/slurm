# ---- 0) Stop & prep ----
sudo systemctl stop slurmd
sudo mkdir -p /etc/slurm /var/spool/slurmd /var/log/slurm
sudo chown -R slurm:slurm /var/spool/slurmd /var/log/slurm

# Ensure hostname resolves to the LAN IP (not 127.0.1.1)
sudo sed -i '/\blambda-hyperplane\b/d' /etc/hosts
echo "192.168.100.25 lambda-hyperplane" | sudo tee -a /etc/hosts

# ---- 1) Make sure MUNGE is healthy (auth) ----
sudo /usr/sbin/create-munge-key -r 2>/dev/null || true
sudo systemctl enable --now munge
munge -n | unmunge >/dev/null || { echo "MUNGE failed"; exit 1; }

# ---- 2) Minimal slurm.conf that matches your host (no cgroups yet) ----
HOST=lambda-hyperplane
IP=192.168.100.25
CPUS=$(nproc)
MEM_MB=$(awk '/MemTotal/{print int($2/1024)}' /proc/meminfo)
GPUS=$(nvidia-smi -L 2>/dev/null | wc -l)

sudo tee /etc/slurm/slurm.conf >/dev/null <<EOF
ClusterName=h100lab
SlurmctldHost=$HOST
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmctld
SlurmdSpoolDir=/var/spool/slurmd
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmctldDebug=info
SlurmdDebug=info
ReturnToService=2

# Keep startup simple first
ProctrackType=proctrack/linuxproc
TaskPlugin=task/none

SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
GresTypes=gpu

NodeName=$HOST NodeAddr=$IP CPUs=$CPUS RealMemory=$MEM_MB Gres=gpu:$GPUS State=UNKNOWN
PartitionName=main Nodes=$HOST Default=YES MaxTime=7-00:00:00 State=UP
EOF

# GPUs via NVML
echo "AutoDetect=nvml" | sudo tee /etc/slurm/gres.conf >/dev/null

# Some distros add bad flags; clear them
sudo sed -i 's/^SLURMD_OPTIONS=.*/SLURMD_OPTIONS=""/' /etc/default/slurmd 2>/dev/null || true

# ---- 3) Start daemons & show errors if any ----
sudo systemctl restart slurmctld
sudo systemctl restart slurmd || true

echo "=== slurmd active? ==="
systemctl is-active slurmd || true
echo "=== last slurmd log ==="
sudo journalctl -u slurmd -n 60 --no-pager
