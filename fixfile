# 0) Stop daemons (safe to run even if down)
sudo systemctl stop slurmctld slurmd

# 1) MUNGE (auth) must exist & run
sudo /usr/sbin/create-munge-key -r 2>/dev/null || true
sudo systemctl enable --now munge

# 2) Dirs + ownership
sudo mkdir -p /var/spool/slurmctld /var/spool/slurmd /var/log/slurm /etc/slurm
sudo chown -R slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm

# 3) Make sure hostname resolves to your LAN IP (not 127.0.1.1)
sudo sed -i '/\blambda-hyperplane\b/d' /etc/hosts
echo "192.168.100.25 lambda-hyperplane" | sudo tee -a /etc/hosts
getent hosts lambda-hyperplane

# 4) Clean, minimal configs (single node, GPUs autodetected)
sudo bash -lc '
HOST=lambda-hyperplane
IP=192.168.100.25
CPUS=$(nproc)
MEM_MB=$(awk "/MemTotal/ {print int(\$2/1024)}" /proc/meminfo)
GPUS=$(nvidia-smi -L 2>/dev/null | wc -l)

cat > /etc/slurm/slurm.conf <<EOF
ClusterName=h100lab
SlurmctldHost=$HOST
# (intentionally not setting ControlMachine to avoid the warning)
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmctld
SlurmdSpoolDir=/var/spool/slurmd
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmctldDebug=info
SlurmdDebug=info
ReturnToService=2

# Keep cgroups simple first; we can harden later
ProctrackType=proctrack/linuxproc
TaskPlugin=task/none
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
GresTypes=gpu

NodeName=$HOST NodeAddr=$IP CPUs=$CPUS RealMemory=$MEM_MB Gres=gpu:$GPUS State=UNKNOWN
PartitionName=debug Nodes=$HOST Default=YES MaxTime=7-00:00:00 State=UP
EOF

echo "AutoDetect=nvml" > /etc/slurm/gres.conf
'

# 5) Some distros add extra flags; clear them
sudo sed -i 's/^SLURMD_OPTIONS=.*/SLURMD_OPTIONS=""/' /etc/default/slurmd 2>/dev/null || true

# 6) Start daemons
sudo systemctl restart slurmctld slurmd

# 7) Health checks
scontrol ping
systemctl is-active slurmd
sinfo -o "%P %N %T %C %m %G"
scontrol show node $(hostname -s) | egrep "State=|Gres=|CfgTRES=|Reason="
