cat >/tmp/slurm_fix_all.sh <<'BASH'
#!/usr/bin/env bash
set -euo pipefail

say(){ printf "\n\033[1;34m[%s]\033[0m %s\n" "$(date +%H:%M:%S)" "$*"; }
warn(){ printf "\n\033[1;33m[WARN]\033[0m %s\n" "$*"; }
die(){ printf "\n\033[1;31m[FAIL]\033[0m %s\n\n" "$*"; exit 1; }

# ---------- Detect basics ----------
HOST=$(hostname -s)
# Allow override: SLURM_IP=… ./script
IP=${SLURM_IP:-$(ip -4 route get 1.1.1.1 2>/dev/null | awk '{for(i=1;i<=NF;i++) if ($i=="src"){print $(i+1); exit}}' || true)}
if [[ -z "${IP:-}" ]]; then
  IP=$(ip -4 -br a | awk '$1!="lo" && $2 ~ /UP/ {split($3,a,"/"); print a[1]; exit}')
fi
[[ -n "${IP:-}" ]] || die "Could not determine node IPv4 address. Set SLURM_IP=<your.IP> and re-run."

CPUS=$(nproc || echo 1)
MEM_MB=$(awk '/MemTotal/ {print int($2/1024)}' /proc/meminfo)
GPU_COUNT=$( (nvidia-smi -L 2>/dev/null | grep -c '^GPU ') || true )
if [[ "$GPU_COUNT" -eq 0 && -e /dev/nvidia0 ]]; then
  GPU_COUNT=$(ls /dev/nvidia[0-9]* 2>/dev/null | wc -w | awk '{print ($1>0)?$1:0}')
fi

say "Host: $HOST  IP: $IP  CPUs: $CPUS  Mem(MiB): $MEM_MB  GPUs: $GPU_COUNT"

# ---------- Packages ----------
pkg_install() {
  if command -v apt >/dev/null 2>&1; then
    sudo apt update
    sudo apt install -y slurm-wlm slurmctld slurmd munge || die "apt install failed"
  elif command -v dnf >/dev/null 2>&1; then
    sudo dnf install -y slurm slurmctld slurm-slurmd munge || die "dnf install failed"
  else
    warn "Unknown distro: please install slurm + munge manually."
  fi
}
pkg_install

# Ensure slurm user exists (usually created by packages)
id -u slurm >/dev/null 2>&1 || sudo useradd -r -M -s /usr/sbin/nologin slurm

# ---------- Auth: MUNGE ----------
say "Ensuring MUNGE key/service"
sudo mkdir -p /etc/munge
[[ -s /etc/munge/munge.key ]] || sudo /usr/sbin/create-munge-key -r
sudo systemctl enable --now munge

# ---------- Directories / ownership ----------
say "Preparing Slurm directories"
sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
sudo chown -R slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm

# ---------- /etc/hosts mapping ----------
say "Mapping $HOST -> $IP in /etc/hosts"
sudo sed -i "/[[:space:]]$HOST\$/d" /etc/hosts
echo "$IP $HOST" | sudo tee -a /etc/hosts >/dev/null

# ---------- Configs ----------
say "Writing /etc/slurm/slurm.conf"
sudo bash -lc "cat >/etc/slurm/slurm.conf" <<EOF
# ===== Minimal single-node Slurm (controller + compute) =====
ClusterName=h100lab
SlurmctldHost=$HOST
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmctld
SlurmdSpoolDir=/var/spool/slurmd
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmctldDebug=info
SlurmdDebug=info
ReturnToService=2

# Keep cgroups simple for first boot; harden later
ProctrackType=proctrack/linuxproc
TaskPlugin=task/none

SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
GresTypes=gpu

NodeName=$HOST NodeAddr=$IP CPUs=$CPUS RealMemory=$MEM_MB Gres=gpu:$GPU_COUNT State=UNKNOWN
PartitionName=main Nodes=$HOST Default=YES MaxTime=7-00:00:00 State=UP
EOF

say "Writing /etc/slurm/gres.conf"
if [[ "$GPU_COUNT" -gt 0 ]]; then
  echo "AutoDetect=nvml" | sudo tee /etc/slurm/gres.conf >/dev/null
else
  warn "No GPUs detected; leaving GRES with gpu:0. GPU jobs will pend."
  echo "" | sudo tee /etc/slurm/gres.conf >/dev/null
fi

# Some distros add flags that break startup; clear them
[[ -f /etc/default/slurmd ]] && sudo sed -i 's/^SLURMD_OPTIONS=.*/SLURMD_OPTIONS=""/' /etc/default/slurmd || true

# ---------- Firewall (if enabled) ----------
if command -v ufw >/dev/null 2>&1 && sudo ufw status | grep -q active; then
  say "Opening ports 6817/6818 via ufw"
  sudo ufw allow 6817/tcp || true
  sudo ufw allow 6818/tcp || true
fi

# ---------- Start services ----------
say "Starting Slurm daemons"
sudo systemctl daemon-reload
sudo systemctl enable --now slurmctld || true
sudo systemctl enable --now slurmd || true

sleep 1
if ! systemctl is-active --quiet slurmctld; then
  warn "slurmctld not active; showing last log lines:"
  sudo tail -n 60 /var/log/slurm/slurmctld.log || true
  sudo journalctl -u slurmctld -n 60 --no-pager || true
  die "slurmctld failed to start"
fi
if ! systemctl is-active --quiet slurmd; then
  warn "slurmd not active; showing last log lines:"
  sudo tail -n 60 /var/log/slurm/slurmd.log || true
  sudo journalctl -u slurmd -n 60 --no-pager || true
  die "slurmd failed to start"
fi

# ---------- Health checks ----------
say "Controller ping"
scontrol ping || die "Controller unreachable"

say "Node status"
sinfo -o "%P %N %T %C %m %G" || true
scontrol show node "$HOST" | egrep "State=|Gres=|CfgTRES=|Reason=" || true

# Node might be in DOWN/DRAIN after first boot; resume it
STATE=$(scontrol show node "$HOST" | sed -n 's/.*State=\([^ ]*\).*/\1/p' | head -n1 || true)
if [[ "$STATE" != "IDLE" && "$STATE" != "ALLOCATED" ]]; then
  say "Resuming node"
  sudo scontrol update NodeName="$HOST" State=RESUME Reason=
fi

# ---------- Smoke test ----------
say "Submitting GPU smoke test (nvidia-smi) to 'main'"
if [[ "$GPU_COUNT" -gt 0 ]]; then
  JOB=$(sbatch -p main --gres=gpu:1 --parsable --wrap="hostname; nvidia-smi -L; echo '---'; nvidia-smi") || die "sbatch failed"
else
  JOB=$(sbatch -p main --parsable --wrap="hostname; echo 'No GPU present'; lscpu | head -n5") || die "sbatch failed"
fi
say "Submitted job $JOB. Waiting for it to finish..."
for i in {1..120}; do
  ST=$(sacct -j "$JOB" --noheader -o state 2>/dev/null | head -n1 | tr -d ' ')
  [[ -z "$ST" ]] && ST=$(squeue -j "$JOB" -h -o %T 2>/dev/null || true)
  [[ "$ST" == "COMPLETED" ]] && break
  [[ "$ST" == "FAILED" || "$ST" == "CANCELLED" || "$ST" == "TIMEOUT" ]] && break
  sleep 1
done

OUT="slurm-${JOB}.out"
if [[ -f "$OUT" ]]; then
  say "Job output:"
  tail -n +1 "$OUT"
else
  warn "Could not find $OUT yet. Current queue:"
  squeue -u "$USER" || true
fi

say "All done ✅"
BASH
sudo bash /tmp/slurm_fix_all.sh
