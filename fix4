# 0) Stop node daemon and prep dirs/ownership
sudo systemctl stop slurmd
sudo mkdir -p /etc/slurm /var/spool/slurmd /var/log/slurm
sudo chown -R slurm:slurm /var/spool/slurmd /var/log/slurm
sudo rm -rf /var/spool/slurmd/*   # safe on single node

# 1) Ensure hostname resolves BOTH to 127.0.1.1 and to your LAN IP
H=$(hostname -s)
IP=192.168.100.25
sudo sed -i "/[[:space:]]$H\$/d" /etc/hosts
echo "127.0.1.1 $H"      | sudo tee -a /etc/hosts >/dev/null
echo "$IP $H"            | sudo tee -a /etc/hosts >/dev/null

# 2) Write a clean slurm.conf with explicit topology + IPv6 off
SOCKETS=$(lscpu | awk -F: '/Socket\(s\)/{gsub(/ /,"",$2);print $2}')
CPS=$(lscpu    | awk -F: '/Core\(s\) per socket/{gsub(/ /,"",$2);print $2}')
TPC=$(lscpu    | awk -F: '/Thread\(s\) per core/{gsub(/ /,"",$2);print $2}')
MEM=$(awk '/MemTotal/{print int($2/1024)}' /proc/meminfo)
GPUS=$(nvidia-smi -L 2>/dev/null | grep -c '^GPU ' || true)

sudo tee /etc/slurm/slurm.conf >/dev/null <<EOF
ClusterName=h100lab
SlurmctldHost=$H
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
SlurmUser=slurm
StateSaveLocation=/var/spool/slurmctld
SlurmdSpoolDir=/var/spool/slurmd
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmctldDebug=info
SlurmdDebug=info
ReturnToService=2
CommunicationParameters=EnableIPv6=No,NoAddrCache

# keep startup simple
ProctrackType=proctrack/linuxproc
TaskPlugin=task/none

SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory
GresTypes=gpu

NodeName=$H NodeAddr=$IP Sockets=$SOCKETS CoresPerSocket=$CPS ThreadsPerCore=$TPC RealMemory=$MEM Gres=gpu:$GPUS State=UNKNOWN
PartitionName=main Nodes=$H Default=YES MaxTime=7-00:00:00 State=UP
EOF

# 3) **Disable NVML autodetect** and list GPU device files explicitly
DEVS=$(ls /dev/nvidia[0-9]* 2>/dev/null | paste -sd, -)
[ -n "$DEVS" ] && sudo tee /etc/slurm/gres.conf >/dev/null <<EOF
NodeName=$H Name=gpu File=$DEVS
EOF

# 4) Make sure Munge works (auth)
sudo systemctl enable --now munge
munge -n | unmunge >/dev/null || echo "MUNGE self-test FAILED"

# 5) Restart daemons
sudo systemctl restart slurmctld
sudo systemctl start slurmd

# 6) Health
systemctl is-active slurmd
sinfo -o "%P %N %T %G %C %m"
scontrol show node $H | egrep "State=|Gres=|Sockets=|CoresPerSocket=|ThreadsPerCore="
